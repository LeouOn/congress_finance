{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "def pre_process(session):\n",
    "    raw_path = 'raw/'\n",
    "    speech_tab = f'{raw_path}speeches_{session}.tab'\n",
    "    speaker_path = 'speaker_names/'\n",
    "    house_path = 'house/'\n",
    "    senate_path = 'senate/'\n",
    "    house_xlsx = f'House{session}Speakers.xlsx'\n",
    "    senate_xlsx = f'Senate{session}Speakers.xlsx'\n",
    "    rt = pd.read_csv(speech_tab, sep = '\\t')\n",
    "    rt.loc[:,'doc_id'] = speech_tab\n",
    "    rt.speaker = rt.speaker.str.lower()\n",
    "    rt.speaker = rt.speaker.str.replace('[^\\w\\s]','')\n",
    "    house = pd.read_excel(os.path.join(speaker_path,house_path,house_xlsx))\n",
    "    house.speaker = house.speaker.str.lower()\n",
    "    house_data = pd.merge(left=rt,right=house,on='speaker')\n",
    "    senate = pd.read_excel(os.path.join(speaker_path,senate_path,senate_xlsx))\n",
    "    senate.speaker = senate.speaker.str.lower()\n",
    "    senate_data = pd.merge(left=rt, right=senate, on='speaker')\n",
    "    rt = rt[~rt.speaker.isin(house.speaker) & ~rt.speaker.isin(senate.speaker)]\n",
    "    return rt, house_data, senate_data\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def tokenize_lemmatize(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        token = lemmatize_stemming(token)\n",
    "        if token not in stoplist and len(token)>3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blingfire as bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = pd.DataFrame()\n",
    "house_data = pd.DataFrame()\n",
    "senate_data = pd.DataFrame()\n",
    "for i in range(103,105):\n",
    "    rt_temp, house_data_temp, senate_data_temp = pre_process(i)\n",
    "    rt = pd.concat([rt,rt_temp])\n",
    "    house_data = pd.concat([house_data, house_data_temp])\n",
    "    senate_data = pd.concat([senate_data, senate_data_temp])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rt, house_data = pre_process(103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contain only speech\n",
    "house_data = house_data[house_data.speech.str.len()>8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legislator and Congress\n",
    "house_data = house_data.groupby(['Legislator ID', 'Congress', 'District', 'Party', 'State ID', 'speaker' \n",
    "                                #, 'day', 'month', 'year'\n",
    "                                ])['speech'].apply(','.join).reset_index()\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stoplist = [\"absent\", \"amend\", \"adjourn\", \"ask\", \"can\", \"chairman\",\n",
    "              \"committee\", \"con\", \"democrat\", \"etc\", \"gentleladies\",\n",
    "              \"gentlelady\",\"gentleman\",\"gentlemen\",\"gentlewoman\",\"gentlewomen\",\n",
    "              \"hereabout\",\"hereafter\",\"hereat\",\"hereby\",\"herein\",\n",
    "              \"hereinafter\",\"hereinbefore\",\"hereinto\",\"hereof\",\"hereon\",\n",
    "              \"hereto\",\"heretofore\",\"hereunder\",\"hereunto\",\"hereupon\",\n",
    "              \"herewith\",\"madam\", \"month\",\"move\",\"mr\",\"mrs\",\"nai\",\"nay\",\"none\",\"now\",\"part\",\"per\", \"peopl\",\n",
    "              \"pro\",\"rise\",\"republican\",\"say\",\"senator\",\"shall\",\"sir\",\"speak\",\n",
    "              \"speaker\",\"tell\",\"thank\",\"thereabout\",\"thereafter\",\"thereagainst\",\"thereat\",\"therebefore\",\n",
    "              \"therebeforn\",\"thereby\",\"therefor\",\"therefore\",\"therefrom\",\n",
    "              \"therein\",\"thereinafter\",\"thereof\",\"thereon\",\"thereto\",\"theretofore\",\n",
    "              \"thereunder\",\"thereunto\",\"thereupon\",\"therewith\",\n",
    "              \"therewithal\",\"time\", \"today\",\"use\",\"whereabouts\",\"whereafter\",\"whereas\",\n",
    "              \"whereat\",\"whereby\",\"wherefore\",\"wherefrom\",\"wherein\",\n",
    "              \"whereinto\",\"whereof\",\"whereon\",\"whereto\",\"whereunder\",\n",
    "              \"whereupon\",\"wherever\",\"wherewith\",\"wherewithal\",\"will\",\n",
    "              \"yea\",\"year\",\"yes\",\"yield\"]\n",
    "\n",
    "stoplist += list(gensim.parsing.preprocessing.STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bf_token(text):\n",
    "    return bf.text_to_words(text).split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [correct, consum, brook, permiss, revis, exten...\n",
       "1    [dummi, prop, befor, want, know, critic, fello...\n",
       "2                     [minut, distinguish, york, forb]\n",
       "3    [good, friend, becaus, like, attent, good, fri...\n",
       "4    [extraordinari, event, hear, whatsoev, airport...\n",
       "5                                   [dingel, michigan]\n",
       "6    [hear, great, deal, talk, differ, better, cong...\n",
       "7    [join, colleagu, fellow, veteran, lane, evan, ...\n",
       "8    [implement, author, feder, trade, commiss, col...\n",
       "9    [strong, opposit, real, onli, hous, fail, cons...\n",
       "Name: speech, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_data.head(10).speech.apply(tokenize_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [The Chairman is correct . ,  Mr . Chairman , ...\n",
       "1    [Mr . Speaker ,  I do n't have any dummies or ...\n",
       "2    [Mr . Speaker ,  I yield 4 minutes to the dist...\n",
       "3    [Mr . Speaker ,  I thank my good friend for yi...\n",
       "4    [Mr . Chairman ,  this is a most extraordinary...\n",
       "5                         [Mr . Dingell of Michigan .]\n",
       "6    [Mr . Speaker ,  I have heard a great deal of ...\n",
       "7    [Mr . Speaker ,  I rise to join my colleague a...\n",
       "8    [Madam Speaker ,  H.R. 395 ,  the Do - Not - C...\n",
       "9    [Mr . Chairman ,  I rise in strong opposition ...\n",
       "Name: speech, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house_data.head(10).speech.apply(bf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'chairman',\n",
       " 'is',\n",
       " 'correct',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'yield',\n",
       " 'myself',\n",
       " 'such',\n",
       " 'time',\n",
       " 'as',\n",
       " 'may',\n",
       " 'consume',\n",
       " 'mr',\n",
       " 'brooks',\n",
       " 'asked',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'permission',\n",
       " 'to',\n",
       " 'revise',\n",
       " 'and',\n",
       " 'extend',\n",
       " 'his',\n",
       " 'remarks',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'opposition',\n",
       " 'to',\n",
       " 'this',\n",
       " 'amendment',\n",
       " 'offered',\n",
       " 'by',\n",
       " 'my',\n",
       " 'good',\n",
       " 'friend',\n",
       " 'and',\n",
       " 'distinguished',\n",
       " 'member',\n",
       " 'of',\n",
       " 'the',\n",
       " 'committee',\n",
       " 'mr',\n",
       " 'ramstad',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'from',\n",
       " 'minnesota',\n",
       " 'because',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'already',\n",
       " 'has',\n",
       " 'the',\n",
       " 'power',\n",
       " 'to',\n",
       " 'remove',\n",
       " 'any',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'for',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'this',\n",
       " 'amendment',\n",
       " 'is',\n",
       " 'unnecessary',\n",
       " 'but',\n",
       " 'of',\n",
       " 'equally',\n",
       " 'great',\n",
       " 'concern',\n",
       " 'to',\n",
       " 'me',\n",
       " 'is',\n",
       " 'that',\n",
       " 'this',\n",
       " 'amendment',\n",
       " 'spells',\n",
       " 'out',\n",
       " 'two',\n",
       " 'but',\n",
       " 'only',\n",
       " 'two',\n",
       " 'of',\n",
       " 'the',\n",
       " 'grounds',\n",
       " 'which',\n",
       " 'might',\n",
       " 'constitute',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'under',\n",
       " 'the',\n",
       " 'statute',\n",
       " 'because',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'for',\n",
       " 'removal',\n",
       " 'could',\n",
       " 'be',\n",
       " 'based',\n",
       " 'on',\n",
       " 'any',\n",
       " 'number',\n",
       " 'of',\n",
       " 'actions',\n",
       " 'misdeeds',\n",
       " 'or',\n",
       " 'circumstances',\n",
       " 'the',\n",
       " 'statute',\n",
       " 'has',\n",
       " 'wisely',\n",
       " 'left',\n",
       " 'the',\n",
       " 'determination',\n",
       " 'of',\n",
       " 'what',\n",
       " 'constitutes',\n",
       " 'the',\n",
       " 'standard',\n",
       " 'of',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'of',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'do',\n",
       " 'so',\n",
       " 'on',\n",
       " 'more',\n",
       " 'technical',\n",
       " 'ground',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'on',\n",
       " 'the',\n",
       " 'surface',\n",
       " 'appears',\n",
       " 'to',\n",
       " 'repeat',\n",
       " 'the',\n",
       " 'scheme',\n",
       " 'that',\n",
       " 'is',\n",
       " 'currently',\n",
       " 'in',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'statute',\n",
       " 'but',\n",
       " 'by',\n",
       " 'using',\n",
       " 'different',\n",
       " 'words',\n",
       " 'it',\n",
       " 'could',\n",
       " 'lead',\n",
       " 'to',\n",
       " 'interpretive',\n",
       " 'confusion',\n",
       " 'very',\n",
       " 'much',\n",
       " 'respect',\n",
       " 'the',\n",
       " 'motivation',\n",
       " 'behind',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'amendment',\n",
       " 'but',\n",
       " 'urge',\n",
       " 'that',\n",
       " 'we',\n",
       " 'keep',\n",
       " 'the',\n",
       " 'statute',\n",
       " 'current',\n",
       " 'treatment',\n",
       " 'of',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'in',\n",
       " 'place',\n",
       " 'for',\n",
       " 'this',\n",
       " 'reason',\n",
       " 'must',\n",
       " 'urge',\n",
       " 'rejection',\n",
       " 'of',\n",
       " 'this',\n",
       " 'amendment',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'reserve',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'of',\n",
       " 'my',\n",
       " 'time',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'will',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'yield',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'to',\n",
       " 'my',\n",
       " 'distinguished',\n",
       " 'friend',\n",
       " 'from',\n",
       " 'minnesota',\n",
       " 'say',\n",
       " 'this',\n",
       " 'section',\n",
       " 'is',\n",
       " 'included',\n",
       " 'in',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'substitute',\n",
       " 'and',\n",
       " 'would',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'we',\n",
       " 'could',\n",
       " 'resolve',\n",
       " 'it',\n",
       " 'in',\n",
       " 'that',\n",
       " 'overall',\n",
       " 'context',\n",
       " 'and',\n",
       " 'not',\n",
       " 'in',\n",
       " 'long',\n",
       " 'separate',\n",
       " 'vote',\n",
       " 'in',\n",
       " 'contention',\n",
       " 'here',\n",
       " 'on',\n",
       " 'the',\n",
       " 'floor',\n",
       " 'we',\n",
       " 'have',\n",
       " 'got',\n",
       " 'three',\n",
       " 'or',\n",
       " 'four',\n",
       " 'at',\n",
       " 'least',\n",
       " 'additional',\n",
       " 'votes',\n",
       " 'on',\n",
       " 'this',\n",
       " 'bill',\n",
       " 'before',\n",
       " 'we',\n",
       " 'conclude',\n",
       " 'this',\n",
       " 'afternoon',\n",
       " 'and',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'members',\n",
       " 'are',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'depart',\n",
       " 'from',\n",
       " 'this',\n",
       " 'city',\n",
       " 'by',\n",
       " 'plane',\n",
       " 'early',\n",
       " 'before',\n",
       " 'that',\n",
       " 'snow',\n",
       " 'storm',\n",
       " 'hits',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'thank',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'will',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'yield',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'would',\n",
       " 'just',\n",
       " 'say',\n",
       " 'that',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'something',\n",
       " 'that',\n",
       " 'we',\n",
       " 'might',\n",
       " 'consider',\n",
       " 'in',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'in',\n",
       " 'other',\n",
       " 'words',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'opposed',\n",
       " 'to',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'and',\n",
       " 'hope',\n",
       " 'we',\n",
       " 'can',\n",
       " 'beat',\n",
       " 'it',\n",
       " 'but',\n",
       " 'that',\n",
       " 'does',\n",
       " 'not',\n",
       " 'mean',\n",
       " 'we',\n",
       " 'will',\n",
       " 'exclude',\n",
       " 'this',\n",
       " 'concept',\n",
       " 'from',\n",
       " 'consideration',\n",
       " 'in',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'yield',\n",
       " 'myself',\n",
       " 'such',\n",
       " 'time',\n",
       " 'as',\n",
       " 'may',\n",
       " 'consume',\n",
       " 'yield',\n",
       " 'to',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'from',\n",
       " 'minnesota',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'let',\n",
       " 'me',\n",
       " 'reclaim',\n",
       " 'my',\n",
       " 'time',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'answer',\n",
       " 'the',\n",
       " 'question',\n",
       " 'briefly',\n",
       " 'it',\n",
       " 'is',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'if',\n",
       " 'you',\n",
       " 'limit',\n",
       " 'it',\n",
       " 'to',\n",
       " 'just',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'issues',\n",
       " 'but',\n",
       " 'there',\n",
       " 'might',\n",
       " 'be',\n",
       " 'several',\n",
       " 'more',\n",
       " 'that',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'might',\n",
       " 'well',\n",
       " 'consider',\n",
       " 'good',\n",
       " 'cause',\n",
       " 'and',\n",
       " 'would',\n",
       " 'rather',\n",
       " 'have',\n",
       " 'the',\n",
       " 'broader',\n",
       " 'interpretation',\n",
       " 'available',\n",
       " 'to',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'that',\n",
       " 'is',\n",
       " 'really',\n",
       " 'my',\n",
       " 'only',\n",
       " 'real',\n",
       " 'query',\n",
       " 'or',\n",
       " 'question',\n",
       " 'about',\n",
       " 'rewriting',\n",
       " 'the',\n",
       " 'language',\n",
       " 'that',\n",
       " 'is',\n",
       " 'what',\n",
       " 'we',\n",
       " 'do',\n",
       " 'not',\n",
       " 'want',\n",
       " 'to',\n",
       " 'do',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'yield',\n",
       " 'back',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'of',\n",
       " 'my',\n",
       " 'time',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'opposition',\n",
       " 'to',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'yield',\n",
       " 'myself',\n",
       " 'such',\n",
       " 'time',\n",
       " 'as',\n",
       " 'may',\n",
       " 'require',\n",
       " 'mr',\n",
       " 'brooks',\n",
       " 'asked',\n",
       " 'and',\n",
       " 'was',\n",
       " 'given',\n",
       " 'permission',\n",
       " 'to',\n",
       " 'revise',\n",
       " 'and',\n",
       " 'extend',\n",
       " 'his',\n",
       " 'remarks',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'must',\n",
       " 'rise',\n",
       " 'in',\n",
       " 'strong',\n",
       " 'opposition',\n",
       " 'to',\n",
       " 'this',\n",
       " 'amendment',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'has',\n",
       " 'two',\n",
       " 'different',\n",
       " 'parts',\n",
       " 'which',\n",
       " 'for',\n",
       " 'some',\n",
       " 'reason',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'be',\n",
       " 'obscured',\n",
       " 'by',\n",
       " 'the',\n",
       " 'sponsors',\n",
       " 'in',\n",
       " 'describing',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'now',\n",
       " 'that',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'of',\n",
       " 'truth',\n",
       " 'has',\n",
       " 'arrived',\n",
       " 'it',\n",
       " 'is',\n",
       " 'essential',\n",
       " 'that',\n",
       " 'all',\n",
       " 'members',\n",
       " 'understand',\n",
       " 'what',\n",
       " 'both',\n",
       " 'parts',\n",
       " 'would',\n",
       " 'do',\n",
       " 'to',\n",
       " 'the',\n",
       " 'structure',\n",
       " 'of',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'process',\n",
       " 'it',\n",
       " 'is',\n",
       " 'understandable',\n",
       " 'why',\n",
       " 'the',\n",
       " 'sponsors',\n",
       " 'of',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'emphasized',\n",
       " 'only',\n",
       " 'the',\n",
       " 'first',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'for',\n",
       " 'that',\n",
       " 'part',\n",
       " 'is',\n",
       " 'nothing',\n",
       " 'more',\n",
       " 'than',\n",
       " 'restatement',\n",
       " 'of',\n",
       " 'the',\n",
       " 'existing',\n",
       " 'standard',\n",
       " 'found',\n",
       " 'in',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'statute',\n",
       " 'that',\n",
       " 'guides',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'in',\n",
       " 'conducting',\n",
       " 'preliminary',\n",
       " 'investigation',\n",
       " 'it',\n",
       " 'was',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bill',\n",
       " 'in',\n",
       " 'thus',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'requires',\n",
       " 'that',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'in',\n",
       " 'determining',\n",
       " 'whether',\n",
       " 'there',\n",
       " 'are',\n",
       " 'grounds',\n",
       " 'to',\n",
       " 'conduct',\n",
       " 'preliminary',\n",
       " 'investigation',\n",
       " 'find',\n",
       " 'that',\n",
       " 'the',\n",
       " 'information',\n",
       " 'submitted',\n",
       " 'to',\n",
       " 'her',\n",
       " 'is',\n",
       " 'specific',\n",
       " 'and',\n",
       " 'from',\n",
       " 'credible',\n",
       " 'source',\n",
       " 'it',\n",
       " 'sounds',\n",
       " 'good',\n",
       " 'guess',\n",
       " 'what',\n",
       " 'the',\n",
       " 'existing',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'statute',\n",
       " 'states',\n",
       " 'the',\n",
       " 'following',\n",
       " 'in',\n",
       " 'determining',\n",
       " 'whether',\n",
       " 'grounds',\n",
       " 'to',\n",
       " 'investigate',\n",
       " 'exist',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'shall',\n",
       " 'consider',\n",
       " 'only',\n",
       " 'the',\n",
       " 'specificity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'information',\n",
       " 'received',\n",
       " 'and',\n",
       " 'the',\n",
       " 'credibility',\n",
       " 'of',\n",
       " 'the',\n",
       " 'source',\n",
       " 'of',\n",
       " 'the',\n",
       " 'information',\n",
       " 'in',\n",
       " 'other',\n",
       " 'words',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'same',\n",
       " 'if',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'was',\n",
       " 'simply',\n",
       " 'restatement',\n",
       " 'of',\n",
       " 'the',\n",
       " 'existing',\n",
       " 'standard',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'superfluous',\n",
       " 'but',\n",
       " 'nothing',\n",
       " 'more',\n",
       " 'but',\n",
       " 'it',\n",
       " 'is',\n",
       " 'something',\n",
       " 'more',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'second',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'that',\n",
       " 'part',\n",
       " 'creates',\n",
       " 'new',\n",
       " 'untested',\n",
       " 'legal',\n",
       " 'standard',\n",
       " 'which',\n",
       " 'eviscerates',\n",
       " 'the',\n",
       " 'very',\n",
       " 'independence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'once',\n",
       " 'he',\n",
       " 'or',\n",
       " 'she',\n",
       " 'is',\n",
       " 'appointed',\n",
       " 'hyde',\n",
       " 'part',\n",
       " 'two',\n",
       " 'as',\n",
       " 'shall',\n",
       " 'call',\n",
       " 'it',\n",
       " 'directs',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'not',\n",
       " 'to',\n",
       " 'proceed',\n",
       " 'with',\n",
       " 'the',\n",
       " 'process',\n",
       " 'if',\n",
       " 'within',\n",
       " 'days',\n",
       " 'she',\n",
       " 'determines',\n",
       " 'there',\n",
       " 'is',\n",
       " 'insufficient',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'violation',\n",
       " 'of',\n",
       " 'criminal',\n",
       " 'law',\n",
       " 'but',\n",
       " 'requiring',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'to',\n",
       " 'make',\n",
       " 'an',\n",
       " 'ultimate',\n",
       " 'finding',\n",
       " 'of',\n",
       " 'whether',\n",
       " 'there',\n",
       " 'is',\n",
       " 'criminal',\n",
       " 'violation',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'function',\n",
       " 'at',\n",
       " 'the',\n",
       " 'preliminary',\n",
       " 'stage',\n",
       " 'ultimate',\n",
       " 'findings',\n",
       " 'of',\n",
       " 'guilt',\n",
       " 'or',\n",
       " 'not',\n",
       " 'are',\n",
       " 'for',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'to',\n",
       " 'make',\n",
       " 'in',\n",
       " 'other',\n",
       " 'words',\n",
       " 'the',\n",
       " 'second',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'would',\n",
       " 'make',\n",
       " 'the',\n",
       " 'appointment',\n",
       " 'of',\n",
       " 'an',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'mere',\n",
       " 'afterthought',\n",
       " 'since',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'will',\n",
       " 'have',\n",
       " 'already',\n",
       " 'prejudged',\n",
       " 'the',\n",
       " 'likely',\n",
       " 'existence',\n",
       " 'of',\n",
       " 'criminal',\n",
       " 'offense',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'having',\n",
       " 'an',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'if',\n",
       " 'the',\n",
       " 'attorney',\n",
       " 'general',\n",
       " 'is',\n",
       " 'both',\n",
       " 'prosecutor',\n",
       " 'and',\n",
       " 'adjudicator',\n",
       " 'of',\n",
       " 'guilt',\n",
       " 'or',\n",
       " 'innocence',\n",
       " 'how',\n",
       " 'does',\n",
       " 'this',\n",
       " 'type',\n",
       " 'of',\n",
       " 'provision',\n",
       " 'avoid',\n",
       " 'the',\n",
       " 'conflict',\n",
       " 'of',\n",
       " 'interest',\n",
       " 'of',\n",
       " 'the',\n",
       " 'executive',\n",
       " 'branch',\n",
       " 'judging',\n",
       " 'itself',\n",
       " 'for',\n",
       " 'all',\n",
       " 'these',\n",
       " 'reasons',\n",
       " 'must',\n",
       " 'urge',\n",
       " 'you',\n",
       " 'to',\n",
       " 'reject',\n",
       " 'the',\n",
       " 'hyde',\n",
       " 'amendment',\n",
       " 'it',\n",
       " 'started',\n",
       " 'out',\n",
       " 'so',\n",
       " 'promising',\n",
       " 'and',\n",
       " 'unobjectionable',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'end',\n",
       " 'of',\n",
       " 'the',\n",
       " 'road',\n",
       " 'it',\n",
       " 'is',\n",
       " 'radical',\n",
       " 'concept',\n",
       " 'that',\n",
       " 'strips',\n",
       " 'away',\n",
       " 'the',\n",
       " 'very',\n",
       " 'independence',\n",
       " 'of',\n",
       " 'the',\n",
       " 'independent',\n",
       " 'counsel',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'reserve',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'of',\n",
       " 'my',\n",
       " 'time',\n",
       " 'mr',\n",
       " 'chairman',\n",
       " 'will',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'yield',\n",
       " 'but',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'light',\n",
       " 'the',\n",
       " 'gentleman',\n",
       " 'remembers',\n",
       " 'that',\n",
       " 'voted',\n",
       " 'for',\n",
       " 'it',\n",
       " 'ever',\n",
       " 'since',\n",
       " 'but',\n",
       " 'have',\n",
       " 'never',\n",
       " 'deterred',\n",
       " 'from',\n",
       " 'my',\n",
       " 'route',\n",
       " 'since',\n",
       " 'then',\n",
       " 'it',\n",
       " 'is',\n",
       " 'these',\n",
       " 'people',\n",
       " 'that',\n",
       " 'go',\n",
       " 'back',\n",
       " 'and',\n",
       " 'forth',\n",
       " 'that',\n",
       " 'make',\n",
       " 'you',\n",
       " 'nervous',\n",
       " 'now',\n",
       " 'what',\n",
       " 'was',\n",
       " 'going',\n",
       " 'to',\n",
       " 'suggest',\n",
       " 'is',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'the',\n",
       " 'first',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'is',\n",
       " 'useful',\n",
       " 'statement',\n",
       " 'of',\n",
       " 'what',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bill',\n",
       " 'now',\n",
       " 'it',\n",
       " 'is',\n",
       " 'the',\n",
       " 'second',\n",
       " 'part',\n",
       " 'that',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim.utils.simple_preprocess(house_data.loc[0,'speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.loc[:,'processed_speeches'] = house_data.speech.map(tokenize_lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(house_data.loc[:,'processed_speeches'])\n",
    "\n",
    "# eliminate 5% most common and words that appear 10 or fewer times. Also eliminate words that appear in 80% of topics\n",
    "\n",
    "dictionary.filter_extremes(no_below=4, no_above=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_data.loc[:,'bow_corpus'] = [dictionary.doc2bow(speech) for speech in house_data.loc[:,'processed_speeches']]\n",
    "\n",
    "tfidf_model = gensim.models.TfidfModel(house_data.loc[:,'bow_corpus'])\n",
    "\n",
    "house_data.loc[:,'corpus_tfidf'] = tfidf_model[house_data.loc[:,'bow_corpus']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = gensim.models.LdaMulticore(house_data.loc[:,'bow_corpus'], \n",
    "#                                        id2word = dictionary, \n",
    "#                                        passes = 2, \n",
    "#                                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "duration = 1  # seconds\n",
    "freq = 440  # Hz\n",
    "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(house_data.loc[:,'corpus_tfidf'], \n",
    "                                       id2word = dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=4)\n",
    "\n",
    "\n",
    "# rt = rt.drop_duplicates('doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-1ce218eb21c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Topic: {} Word: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"ought\" + 0.003*\"wetland\" + 0.003*\"crime\" + 0.003*\"rule\" + 0.003*\"fund\" + 0.003*\"attorney\" + 0.003*\"court\" + 0.003*\"feder\" + 0.003*\"prison\" + 0.003*\"senat\"\n",
      "\n",
      "\n",
      "Topic: 1 Word: 0.003*\"coleman\" + 0.002*\"tyndal\" + 0.002*\"peanut\" + 0.002*\"waiver\" + 0.002*\"collin\" + 0.002*\"cleveland\" + 0.001*\"african\" + 0.001*\"diego\" + 0.001*\"coastal\" + 0.001*\"bootl\"\n",
      "\n",
      "\n",
      "Topic: 2 Word: 0.004*\"cyprus\" + 0.004*\"greek\" + 0.003*\"peacekeep\" + 0.002*\"turkish\" + 0.002*\"porter\" + 0.002*\"pittsburgh\" + 0.002*\"picatinni\" + 0.002*\"natcher\" + 0.002*\"missil\" + 0.002*\"cypriot\"\n",
      "\n",
      "\n",
      "Topic: 3 Word: 0.003*\"wyom\" + 0.002*\"saif\" + 0.002*\"thrift\" + 0.001*\"graffiti\" + 0.001*\"rainey\" + 0.001*\"ranch\" + 0.001*\"gambl\" + 0.001*\"leas\" + 0.001*\"livestock\" + 0.001*\"bishop\"\n",
      "\n",
      "\n",
      "Topic: 4 Word: 0.002*\"superfund\" + 0.002*\"monterey\" + 0.002*\"salmon\" + 0.002*\"pipelin\" + 0.001*\"plainfield\" + 0.001*\"woodbridg\" + 0.001*\"crime\" + 0.001*\"liabil\" + 0.001*\"secondhand\" + 0.001*\"supermand\"\n",
      "\n",
      "\n",
      "Topic: 5 Word: 0.004*\"agricultur\" + 0.003*\"stamp\" + 0.003*\"rolla\" + 0.003*\"usda\" + 0.002*\"okinawa\" + 0.002*\"food\" + 0.002*\"farm\" + 0.002*\"farmer\" + 0.002*\"winona\" + 0.001*\"ryce\"\n",
      "\n",
      "\n",
      "Topic: 6 Word: 0.003*\"candid\" + 0.002*\"newark\" + 0.002*\"black\" + 0.002*\"kingpin\" + 0.002*\"african\" + 0.001*\"oklahoma\" + 0.001*\"greensboro\" + 0.001*\"africa\" + 0.001*\"edmond\" + 0.001*\"courtney\"\n",
      "\n",
      "\n",
      "Topic: 7 Word: 0.004*\"joliet\" + 0.002*\"space\" + 0.002*\"nasa\" + 0.002*\"doug\" + 0.002*\"tribe\" + 0.002*\"cruis\" + 0.002*\"whitten\" + 0.002*\"longoria\" + 0.001*\"antonio\" + 0.001*\"selena\"\n",
      "\n",
      "\n",
      "Topic: 8 Word: 0.003*\"helium\" + 0.003*\"louisvill\" + 0.002*\"hartford\" + 0.002*\"ward\" + 0.002*\"kentucki\" + 0.002*\"referendum\" + 0.002*\"cuban\" + 0.002*\"bayonn\" + 0.002*\"china\" + 0.001*\"cuba\"\n",
      "\n",
      "\n",
      "Topic: 9 Word: 0.006*\"rhode\" + 0.002*\"island\" + 0.001*\"dona\" + 0.001*\"rican\" + 0.001*\"brad\" + 0.001*\"narragansett\" + 0.001*\"puerto\" + 0.001*\"imet\" + 0.001*\"punit\" + 0.001*\"fayett\"\n",
      "\n",
      "\n",
      "Topic: 10 Word: 0.003*\"coin\" + 0.002*\"steubenvill\" + 0.002*\"culp\" + 0.002*\"secrest\" + 0.002*\"delawar\" + 0.002*\"brien\" + 0.001*\"hobbi\" + 0.001*\"cyprus\" + 0.001*\"mint\" + 0.001*\"firearm\"\n",
      "\n",
      "\n",
      "Topic: 11 Word: 0.004*\"development\" + 0.002*\"frisa\" + 0.001*\"sleev\" + 0.001*\"censor\" + 0.001*\"blah\" + 0.001*\"patent\" + 0.001*\"bliley\" + 0.001*\"disabl\" + 0.001*\"meadow\" + 0.001*\"breast\"\n",
      "\n",
      "\n",
      "Topic: 12 Word: 0.004*\"nuclear\" + 0.003*\"brooklyn\" + 0.002*\"addict\" + 0.002*\"york\" + 0.002*\"cyprus\" + 0.002*\"medicar\" + 0.002*\"citi\" + 0.002*\"women\" + 0.002*\"communiti\" + 0.002*\"brac\"\n",
      "\n",
      "\n",
      "Topic: 13 Word: 0.002*\"nlrb\" + 0.002*\"parliamentari\" + 0.002*\"inquiri\" + 0.002*\"china\" + 0.002*\"amtrak\" + 0.002*\"alto\" + 0.001*\"palo\" + 0.001*\"asylum\" + 0.001*\"detach\" + 0.001*\"veteran\"\n",
      "\n",
      "\n",
      "Topic: 14 Word: 0.003*\"fisheri\" + 0.002*\"china\" + 0.002*\"vessel\" + 0.002*\"flint\" + 0.002*\"bateman\" + 0.002*\"vancouv\" + 0.001*\"texaco\" + 0.001*\"cape\" + 0.001*\"fishermen\" + 0.001*\"maritim\"\n",
      "\n",
      "\n",
      "Topic: 15 Word: 0.004*\"hawaii\" + 0.003*\"weather\" + 0.001*\"nasa\" + 0.001*\"alabama\" + 0.001*\"iter\" + 0.001*\"space\" + 0.001*\"marriag\" + 0.001*\"farrakhan\" + 0.001*\"colbi\" + 0.001*\"eunic\"\n",
      "\n",
      "\n",
      "Topic: 16 Word: 0.003*\"homosexu\" + 0.003*\"sunshin\" + 0.002*\"helium\" + 0.002*\"print\" + 0.002*\"shipbuild\" + 0.002*\"radar\" + 0.001*\"unsoeld\" + 0.001*\"hammond\" + 0.001*\"lincoln\" + 0.001*\"erisa\"\n",
      "\n",
      "\n",
      "Topic: 17 Word: 0.003*\"hamer\" + 0.002*\"medgar\" + 0.002*\"mississippi\" + 0.002*\"chicago\" + 0.001*\"portman\" + 0.001*\"gayl\" + 0.001*\"african\" + 0.001*\"blackwel\" + 0.001*\"gang\" + 0.001*\"eyeglass\"\n",
      "\n",
      "\n",
      "Topic: 18 Word: 0.003*\"burr\" + 0.002*\"hotel\" + 0.002*\"border\" + 0.002*\"crime\" + 0.002*\"diabet\" + 0.002*\"cellular\" + 0.001*\"fulbright\" + 0.001*\"alien\" + 0.001*\"pioneer\" + 0.001*\"arkansa\"\n",
      "\n",
      "\n",
      "Topic: 19 Word: 0.003*\"philadelphia\" + 0.003*\"colorado\" + 0.003*\"quorum\" + 0.002*\"aviat\" + 0.002*\"rule\" + 0.002*\"airlin\" + 0.002*\"park\" + 0.002*\"meehan\" + 0.002*\"milwauke\" + 0.002*\"african\"\n",
      "\n",
      "\n",
      "Topic: 20 Word: 0.003*\"jackson\" + 0.003*\"kristen\" + 0.002*\"alpha\" + 0.002*\"juanita\" + 0.002*\"lans\" + 0.002*\"bifida\" + 0.002*\"spina\" + 0.001*\"predeceas\" + 0.001*\"milwauke\" + 0.001*\"pharmaci\"\n",
      "\n",
      "\n",
      "Topic: 21 Word: 0.003*\"murfreesboro\" + 0.003*\"rotari\" + 0.002*\"firefight\" + 0.002*\"almr\" + 0.002*\"volunt\" + 0.002*\"plutonium\" + 0.002*\"courthous\" + 0.002*\"club\" + 0.001*\"embezzl\" + 0.001*\"ferc\"\n",
      "\n",
      "\n",
      "Topic: 22 Word: 0.003*\"pizza\" + 0.002*\"lupus\" + 0.002*\"louisiana\" + 0.002*\"lasat\" + 0.002*\"peregrin\" + 0.002*\"edit\" + 0.002*\"reactor\" + 0.002*\"meal\" + 0.001*\"springfield\" + 0.001*\"chemic\"\n",
      "\n",
      "\n",
      "Topic: 23 Word: 0.002*\"sugar\" + 0.002*\"buffalo\" + 0.002*\"welfar\" + 0.002*\"tensa\" + 0.001*\"seattl\" + 0.001*\"iniki\" + 0.001*\"salmon\" + 0.001*\"medicar\" + 0.001*\"internet\" + 0.001*\"hanford\"\n",
      "\n",
      "\n",
      "Topic: 24 Word: 0.003*\"passaic\" + 0.003*\"lockbox\" + 0.002*\"paterson\" + 0.002*\"nigeria\" + 0.002*\"lake\" + 0.002*\"hatcheri\" + 0.002*\"starr\" + 0.002*\"arkansa\" + 0.002*\"fish\" + 0.002*\"westland\"\n",
      "\n",
      "\n",
      "Topic: 25 Word: 0.003*\"medgar\" + 0.002*\"lobbyist\" + 0.001*\"waterloo\" + 0.001*\"desert\" + 0.001*\"randal\" + 0.001*\"bighorn\" + 0.001*\"missouri\" + 0.001*\"eagleton\" + 0.001*\"madigan\" + 0.001*\"porch\"\n",
      "\n",
      "\n",
      "Topic: 26 Word: 0.004*\"galleg\" + 0.001*\"mariana\" + 0.001*\"immigr\" + 0.001*\"rabbi\" + 0.001*\"island\" + 0.001*\"illeg\" + 0.001*\"weapon\" + 0.001*\"yeshiva\" + 0.001*\"assault\" + 0.001*\"tibetan\"\n",
      "\n",
      "\n",
      "Topic: 27 Word: 0.003*\"frivol\" + 0.002*\"wetland\" + 0.001*\"cincinnati\" + 0.001*\"suit\" + 0.001*\"farmer\" + 0.001*\"michigan\" + 0.001*\"propan\" + 0.001*\"speci\" + 0.001*\"fresno\" + 0.001*\"lata\"\n",
      "\n",
      "\n",
      "Topic: 28 Word: 0.003*\"taiwan\" + 0.003*\"georgia\" + 0.002*\"medicar\" + 0.002*\"district\" + 0.002*\"kashmir\" + 0.002*\"wage\" + 0.002*\"think\" + 0.002*\"senat\" + 0.002*\"percent\" + 0.002*\"budget\"\n",
      "\n",
      "\n",
      "Topic: 29 Word: 0.003*\"citrus\" + 0.002*\"kruger\" + 0.001*\"alfr\" + 0.001*\"florida\" + 0.001*\"byrn\" + 0.001*\"gator\" + 0.001*\"alien\" + 0.001*\"incarcer\" + 0.001*\"thurman\" + 0.001*\"coach\"\n",
      "\n",
      "\n",
      "Topic: 30 Word: 0.004*\"educ\" + 0.004*\"wage\" + 0.004*\"china\" + 0.004*\"health\" + 0.004*\"school\" + 0.004*\"speci\" + 0.003*\"pension\" + 0.003*\"care\" + 0.003*\"market\" + 0.003*\"invest\"\n",
      "\n",
      "\n",
      "Topic: 31 Word: 0.003*\"missil\" + 0.002*\"wonder\" + 0.002*\"everglad\" + 0.002*\"tucson\" + 0.002*\"morehous\" + 0.002*\"brock\" + 0.002*\"nogal\" + 0.001*\"securit\" + 0.001*\"arizona\" + 0.001*\"wage\"\n",
      "\n",
      "\n",
      "Topic: 32 Word: 0.004*\"collin\" + 0.003*\"flint\" + 0.003*\"mfume\" + 0.002*\"michigan\" + 0.002*\"recycl\" + 0.002*\"african\" + 0.002*\"reactor\" + 0.002*\"bevil\" + 0.002*\"flow\" + 0.002*\"liberia\"\n",
      "\n",
      "\n",
      "Topic: 33 Word: 0.002*\"montana\" + 0.001*\"yellowston\" + 0.001*\"icwa\" + 0.001*\"budget\" + 0.001*\"tacoma\" + 0.001*\"rule\" + 0.001*\"spokan\" + 0.001*\"feder\" + 0.001*\"pension\" + 0.001*\"visa\"\n",
      "\n",
      "\n",
      "Topic: 34 Word: 0.003*\"osha\" + 0.002*\"sikh\" + 0.002*\"armenian\" + 0.002*\"turkey\" + 0.002*\"hyatt\" + 0.001*\"racin\" + 0.001*\"genocid\" + 0.001*\"alabama\" + 0.001*\"pacifica\" + 0.001*\"chattahooche\"\n",
      "\n",
      "\n",
      "Topic: 35 Word: 0.002*\"iowa\" + 0.002*\"haitian\" + 0.002*\"bethun\" + 0.002*\"moor\" + 0.001*\"smart\" + 0.001*\"protestor\" + 0.001*\"cookman\" + 0.001*\"daytona\" + 0.001*\"invas\" + 0.001*\"hatcheri\"\n",
      "\n",
      "\n",
      "Topic: 36 Word: 0.002*\"buchenwald\" + 0.002*\"presidio\" + 0.001*\"documentari\" + 0.001*\"battalion\" + 0.001*\"film\" + 0.001*\"armor\" + 0.001*\"derrick\" + 0.001*\"infantri\" + 0.001*\"pemex\" + 0.001*\"dachau\"\n",
      "\n",
      "\n",
      "Topic: 37 Word: 0.005*\"dade\" + 0.004*\"wednesday\" + 0.003*\"miami\" + 0.003*\"myrick\" + 0.003*\"meek\" + 0.002*\"charlott\" + 0.002*\"gaston\" + 0.001*\"timor\" + 0.001*\"noon\" + 0.001*\"charact\"\n",
      "\n",
      "\n",
      "Topic: 38 Word: 0.003*\"stalker\" + 0.002*\"indianapoli\" + 0.002*\"jacob\" + 0.002*\"fairfax\" + 0.002*\"pocahonta\" + 0.002*\"nelli\" + 0.001*\"opic\" + 0.001*\"brooklyn\" + 0.001*\"stalk\" + 0.001*\"gentil\"\n",
      "\n",
      "\n",
      "Topic: 39 Word: 0.003*\"harman\" + 0.002*\"diego\" + 0.002*\"timber\" + 0.002*\"medicar\" + 0.001*\"salvag\" + 0.001*\"demagogu\" + 0.001*\"budget\" + 0.001*\"tabl\" + 0.001*\"medicaid\" + 0.001*\"lockbox\"\n",
      "\n",
      "\n",
      "Topic: 40 Word: 0.004*\"sikh\" + 0.003*\"milwauke\" + 0.002*\"polish\" + 0.002*\"beer\" + 0.002*\"lowel\" + 0.002*\"gillmor\" + 0.001*\"northwest\" + 0.001*\"club\" + 0.001*\"khalistan\" + 0.001*\"gambl\"\n",
      "\n",
      "\n",
      "Topic: 41 Word: 0.007*\"welfar\" + 0.005*\"budget\" + 0.005*\"clinton\" + 0.004*\"medicar\" + 0.004*\"care\" + 0.004*\"presid\" + 0.004*\"think\" + 0.004*\"balanc\" + 0.004*\"talk\" + 0.004*\"america\"\n",
      "\n",
      "\n",
      "Topic: 42 Word: 0.001*\"ozon\" + 0.001*\"khalistan\" + 0.001*\"reactiv\" + 0.001*\"mandat\" + 0.001*\"sikh\" + 0.001*\"assyrian\" + 0.001*\"joyc\" + 0.001*\"unfund\" + 0.001*\"health\" + 0.001*\"khalra\"\n",
      "\n",
      "\n",
      "Topic: 43 Word: 0.002*\"arkansa\" + 0.002*\"wilk\" + 0.002*\"armi\" + 0.002*\"medicar\" + 0.002*\"jeremiah\" + 0.002*\"greensboro\" + 0.001*\"allotte\" + 0.001*\"medal\" + 0.001*\"command\" + 0.001*\"presid\"\n",
      "\n",
      "\n",
      "Topic: 44 Word: 0.003*\"tribe\" + 0.003*\"ireland\" + 0.002*\"pilt\" + 0.002*\"montana\" + 0.002*\"wilder\" + 0.002*\"hochbrueckn\" + 0.001*\"emerson\" + 0.001*\"brownel\" + 0.001*\"indian\" + 0.001*\"aspirin\"\n",
      "\n",
      "\n",
      "Topic: 45 Word: 0.003*\"peanut\" + 0.002*\"nasa\" + 0.002*\"mussel\" + 0.002*\"psychologist\" + 0.002*\"fort\" + 0.002*\"zebra\" + 0.002*\"tribe\" + 0.001*\"judici\" + 0.001*\"indian\" + 0.001*\"lake\"\n",
      "\n",
      "\n",
      "Topic: 46 Word: 0.003*\"korea\" + 0.002*\"diego\" + 0.002*\"korean\" + 0.002*\"haiti\" + 0.001*\"peabodi\" + 0.001*\"circumst\" + 0.001*\"wetterl\" + 0.001*\"minefield\" + 0.001*\"closur\" + 0.001*\"north\"\n",
      "\n",
      "\n",
      "Topic: 47 Word: 0.003*\"procur\" + 0.002*\"plaintiff\" + 0.002*\"ammunit\" + 0.002*\"counterfeit\" + 0.001*\"alexandria\" + 0.001*\"whitt\" + 0.001*\"saginaw\" + 0.001*\"nonemerg\" + 0.001*\"trainabl\" + 0.001*\"feder\"\n",
      "\n",
      "\n",
      "Topic: 48 Word: 0.005*\"natcher\" + 0.003*\"diego\" + 0.002*\"kentucki\" + 0.001*\"selena\" + 0.001*\"garcia\" + 0.001*\"social\" + 0.001*\"taiwan\" + 0.001*\"oira\" + 0.001*\"moratorium\" + 0.001*\"crime\"\n",
      "\n",
      "\n",
      "Topic: 49 Word: 0.003*\"beer\" + 0.002*\"paso\" + 0.002*\"health\" + 0.002*\"deduct\" + 0.002*\"miner\" + 0.002*\"clinic\" + 0.002*\"springfield\" + 0.002*\"nashvill\" + 0.002*\"blackston\" + 0.002*\"ballast\"\n",
      "\n",
      "\n",
      "Topic: 50 Word: 0.004*\"mateo\" + 0.002*\"riversid\" + 0.002*\"armenian\" + 0.002*\"aviat\" + 0.002*\"presidio\" + 0.002*\"vermont\" + 0.001*\"gari\" + 0.001*\"student\" + 0.001*\"school\" + 0.001*\"apprentic\"\n",
      "\n",
      "\n",
      "Topic: 51 Word: 0.002*\"enrolle\" + 0.002*\"medicar\" + 0.001*\"kentucki\" + 0.001*\"barlow\" + 0.001*\"truste\" + 0.001*\"amput\" + 0.001*\"rollcal\" + 0.001*\"whitten\" + 0.001*\"cost\" + 0.001*\"haiti\"\n",
      "\n",
      "\n",
      "Topic: 52 Word: 0.003*\"rockford\" + 0.002*\"weaver\" + 0.002*\"hampshir\" + 0.002*\"dystonia\" + 0.002*\"farmer\" + 0.002*\"medicar\" + 0.001*\"popcorn\" + 0.001*\"budget\" + 0.001*\"kosova\" + 0.001*\"sexual\"\n",
      "\n",
      "\n",
      "Topic: 53 Word: 0.004*\"troop\" + 0.004*\"deploy\" + 0.004*\"defens\" + 0.003*\"militari\" + 0.003*\"haiti\" + 0.003*\"jami\" + 0.003*\"fowler\" + 0.003*\"reagan\" + 0.002*\"bosnia\" + 0.002*\"missil\"\n",
      "\n",
      "\n",
      "Topic: 54 Word: 0.003*\"salt\" + 0.002*\"tunnel\" + 0.002*\"armenian\" + 0.002*\"mcnulti\" + 0.002*\"limit\" + 0.002*\"tennesse\" + 0.001*\"term\" + 0.001*\"lake\" + 0.001*\"simon\" + 0.001*\"hilleari\"\n",
      "\n",
      "\n",
      "Topic: 55 Word: 0.002*\"oklahoma\" + 0.002*\"cincinnati\" + 0.002*\"adel\" + 0.001*\"habea\" + 0.001*\"cotton\" + 0.001*\"corpus\" + 0.001*\"rule\" + 0.001*\"murrah\" + 0.001*\"aphi\" + 0.001*\"innoc\"\n",
      "\n",
      "\n",
      "Topic: 56 Word: 0.002*\"homeless\" + 0.001*\"indoor\" + 0.001*\"awol\" + 0.001*\"portland\" + 0.001*\"ought\" + 0.001*\"drink\" + 0.001*\"khalistan\" + 0.001*\"oregon\" + 0.001*\"medicar\" + 0.001*\"major\"\n",
      "\n",
      "\n",
      "Topic: 57 Word: 0.004*\"sacramento\" + 0.002*\"houston\" + 0.002*\"birmingham\" + 0.001*\"workforc\" + 0.001*\"cent\" + 0.001*\"conni\" + 0.001*\"salli\" + 0.001*\"student\" + 0.001*\"pardon\" + 0.001*\"decatur\"\n",
      "\n",
      "\n",
      "Topic: 58 Word: 0.003*\"baselin\" + 0.003*\"nicotin\" + 0.002*\"omaha\" + 0.001*\"philip\" + 0.001*\"morri\" + 0.001*\"allergi\" + 0.001*\"barton\" + 0.001*\"cigarett\" + 0.001*\"stenholm\" + 0.001*\"chisholm\"\n",
      "\n",
      "\n",
      "Topic: 59 Word: 0.003*\"sbic\" + 0.002*\"latim\" + 0.002*\"betaseron\" + 0.001*\"fremont\" + 0.001*\"gambl\" + 0.001*\"height\" + 0.001*\"bank\" + 0.001*\"jamaica\" + 0.001*\"archiv\" + 0.001*\"communiti\"\n",
      "\n",
      "\n",
      "Topic: 60 Word: 0.003*\"revis\" + 0.003*\"clerk\" + 0.003*\"resolut\" + 0.003*\"extend\" + 0.003*\"remark\" + 0.003*\"permiss\" + 0.002*\"rule\" + 0.002*\"consider\" + 0.002*\"consent\" + 0.002*\"unanim\"\n",
      "\n",
      "\n",
      "Topic: 61 Word: 0.003*\"kennedi\" + 0.003*\"dalla\" + 0.002*\"greenvill\" + 0.002*\"parish\" + 0.001*\"colorect\" + 0.001*\"african\" + 0.001*\"wednesday\" + 0.001*\"monday\" + 0.001*\"tuesday\" + 0.001*\"veget\"\n",
      "\n",
      "\n",
      "Topic: 62 Word: 0.004*\"budget\" + 0.003*\"medicar\" + 0.003*\"balanc\" + 0.003*\"stenholm\" + 0.002*\"spend\" + 0.002*\"saratoga\" + 0.002*\"billion\" + 0.002*\"glen\" + 0.002*\"lightfoot\" + 0.002*\"deficit\"\n",
      "\n",
      "\n",
      "Topic: 63 Word: 0.002*\"jami\" + 0.001*\"naic\" + 0.001*\"crop\" + 0.001*\"fsis\" + 0.001*\"shrontz\" + 0.001*\"whitten\" + 0.001*\"dakota\" + 0.001*\"usda\" + 0.001*\"wetland\" + 0.001*\"budget\"\n",
      "\n",
      "\n",
      "Topic: 64 Word: 0.002*\"houston\" + 0.002*\"nebraska\" + 0.002*\"colonel\" + 0.001*\"burundi\" + 0.001*\"intellig\" + 0.001*\"screen\" + 0.001*\"employ\" + 0.001*\"hassl\" + 0.001*\"crop\" + 0.001*\"pasadena\"\n",
      "\n",
      "\n",
      "Topic: 65 Word: 0.005*\"scout\" + 0.002*\"cincinnati\" + 0.002*\"ambulatori\" + 0.002*\"eagl\" + 0.002*\"vamc\" + 0.002*\"madigan\" + 0.002*\"pacheco\" + 0.002*\"antonio\" + 0.001*\"park\" + 0.001*\"georgia\"\n",
      "\n",
      "\n",
      "Topic: 66 Word: 0.003*\"helium\" + 0.002*\"opic\" + 0.002*\"commerc\" + 0.002*\"mccomb\" + 0.001*\"african\" + 0.001*\"griffin\" + 0.001*\"budget\" + 0.001*\"depart\" + 0.001*\"magnolia\" + 0.001*\"baptist\"\n",
      "\n",
      "\n",
      "Topic: 67 Word: 0.001*\"buddi\" + 0.001*\"kraus\" + 0.001*\"chet\" + 0.001*\"sergeant\" + 0.001*\"gift\" + 0.001*\"wolfensohn\" + 0.001*\"thrift\" + 0.001*\"carrier\" + 0.001*\"rural\" + 0.000*\"boehner\"\n",
      "\n",
      "\n",
      "Topic: 68 Word: 0.004*\"english\" + 0.003*\"houston\" + 0.002*\"pico\" + 0.002*\"insur\" + 0.002*\"brooklyn\" + 0.002*\"reverend\" + 0.002*\"santa\" + 0.002*\"montebello\" + 0.002*\"payer\" + 0.002*\"coverag\"\n",
      "\n",
      "\n",
      "Topic: 69 Word: 0.003*\"gingrich\" + 0.002*\"care\" + 0.002*\"health\" + 0.002*\"school\" + 0.002*\"budget\" + 0.002*\"black\" + 0.002*\"gephardt\" + 0.002*\"presid\" + 0.002*\"whitewat\" + 0.002*\"newt\"\n",
      "\n",
      "\n",
      "Topic: 70 Word: 0.004*\"think\" + 0.004*\"budget\" + 0.004*\"minut\" + 0.004*\"feder\" + 0.004*\"rule\" + 0.004*\"medicar\" + 0.004*\"spend\" + 0.003*\"presid\" + 0.003*\"percent\" + 0.003*\"million\"\n",
      "\n",
      "\n",
      "Topic: 71 Word: 0.003*\"afdc\" + 0.003*\"medicaid\" + 0.003*\"park\" + 0.003*\"brown\" + 0.003*\"talk\" + 0.003*\"tobacco\" + 0.003*\"welfar\" + 0.003*\"hawaii\" + 0.002*\"veteran\" + 0.002*\"championship\"\n",
      "\n",
      "\n",
      "Topic: 72 Word: 0.004*\"nasa\" + 0.004*\"tobacco\" + 0.003*\"hagerstown\" + 0.002*\"space\" + 0.002*\"perk\" + 0.001*\"helicopt\" + 0.001*\"charlottesvill\" + 0.001*\"coalit\" + 0.001*\"station\" + 0.001*\"flight\"\n",
      "\n",
      "\n",
      "Topic: 73 Word: 0.002*\"jersey\" + 0.002*\"ukrainian\" + 0.001*\"ireland\" + 0.001*\"uganda\" + 0.001*\"dayton\" + 0.001*\"astoria\" + 0.001*\"health\" + 0.001*\"schooler\" + 0.001*\"lindsay\" + 0.001*\"impuls\"\n",
      "\n",
      "\n",
      "Topic: 74 Word: 0.003*\"refug\" + 0.003*\"wilder\" + 0.003*\"alaska\" + 0.002*\"desert\" + 0.002*\"properti\" + 0.002*\"wildlif\" + 0.002*\"land\" + 0.002*\"rancher\" + 0.002*\"tongass\" + 0.002*\"fish\"\n",
      "\n",
      "\n",
      "Topic: 75 Word: 0.006*\"postal\" + 0.002*\"census\" + 0.002*\"predat\" + 0.002*\"submarin\" + 0.002*\"liheap\" + 0.001*\"transport\" + 0.001*\"waterburi\" + 0.001*\"friar\" + 0.001*\"highway\" + 0.001*\"carrier\"\n",
      "\n",
      "\n",
      "Topic: 76 Word: 0.007*\"veteran\" + 0.002*\"copyright\" + 0.002*\"stump\" + 0.002*\"patent\" + 0.002*\"montgomeri\" + 0.002*\"bank\" + 0.002*\"rican\" + 0.002*\"robinson\" + 0.002*\"puerto\" + 0.002*\"pell\"\n",
      "\n",
      "\n",
      "Topic: 77 Word: 0.000*\"tampa\" + 0.000*\"missil\" + 0.000*\"chicago\" + 0.000*\"russian\" + 0.000*\"tobacco\" + 0.000*\"redlin\" + 0.000*\"minut\" + 0.000*\"insur\" + 0.000*\"command\" + 0.000*\"missouri\"\n",
      "\n",
      "\n",
      "Topic: 78 Word: 0.003*\"borrow\" + 0.002*\"workplac\" + 0.002*\"employe\" + 0.002*\"michigan\" + 0.002*\"smith\" + 0.001*\"nlra\" + 0.001*\"wisconsin\" + 0.001*\"employ\" + 0.001*\"union\" + 0.001*\"labor\"\n",
      "\n",
      "\n",
      "Topic: 79 Word: 0.004*\"immigr\" + 0.004*\"alien\" + 0.002*\"budget\" + 0.002*\"vaccin\" + 0.002*\"illeg\" + 0.002*\"senat\" + 0.002*\"deport\" + 0.002*\"medicaid\" + 0.002*\"ought\" + 0.002*\"think\"\n",
      "\n",
      "\n",
      "Topic: 80 Word: 0.003*\"royalti\" + 0.003*\"sheriff\" + 0.002*\"faulkner\" + 0.001*\"lesse\" + 0.001*\"riversid\" + 0.001*\"dora\" + 0.001*\"smokey\" + 0.001*\"poshard\" + 0.001*\"lucian\" + 0.001*\"edgar\"\n",
      "\n",
      "\n",
      "Topic: 81 Word: 0.001*\"microsoft\" + 0.001*\"foia\" + 0.001*\"steelhead\" + 0.001*\"curbsid\" + 0.001*\"everglad\" + 0.001*\"sbic\" + 0.000*\"presid\" + 0.000*\"senat\" + 0.000*\"fernand\" + 0.000*\"forest\"\n",
      "\n",
      "\n",
      "Topic: 82 Word: 0.003*\"amtrak\" + 0.002*\"penn\" + 0.002*\"pilt\" + 0.001*\"transport\" + 0.001*\"haiti\" + 0.001*\"highway\" + 0.001*\"knapp\" + 0.001*\"michigan\" + 0.001*\"carr\" + 0.001*\"veteran\"\n",
      "\n",
      "\n",
      "Topic: 83 Word: 0.002*\"microsoft\" + 0.002*\"yakima\" + 0.002*\"sikh\" + 0.001*\"sportsmen\" + 0.001*\"ugli\" + 0.001*\"basin\" + 0.001*\"park\" + 0.001*\"china\" + 0.001*\"geren\" + 0.001*\"sheep\"\n",
      "\n",
      "\n",
      "Topic: 84 Word: 0.002*\"landown\" + 0.002*\"philadelphia\" + 0.001*\"properti\" + 0.001*\"disney\" + 0.001*\"battlefield\" + 0.001*\"steamtown\" + 0.001*\"tolbert\" + 0.001*\"manassa\" + 0.001*\"african\" + 0.001*\"redlin\"\n",
      "\n",
      "\n",
      "Topic: 85 Word: 0.003*\"deepwat\" + 0.003*\"sacramento\" + 0.001*\"aarp\" + 0.001*\"pinella\" + 0.001*\"boehner\" + 0.001*\"marrow\" + 0.001*\"port\" + 0.001*\"medicar\" + 0.001*\"loop\" + 0.001*\"gulfport\"\n",
      "\n",
      "\n",
      "Topic: 86 Word: 0.004*\"haiti\" + 0.004*\"haitian\" + 0.003*\"aristid\" + 0.002*\"packard\" + 0.002*\"peanut\" + 0.002*\"wisconsin\" + 0.001*\"budget\" + 0.001*\"farmer\" + 0.001*\"bertrand\" + 0.001*\"medicar\"\n",
      "\n",
      "\n",
      "Topic: 87 Word: 0.002*\"diego\" + 0.002*\"mayer\" + 0.002*\"armenian\" + 0.002*\"monterey\" + 0.001*\"alton\" + 0.001*\"hanford\" + 0.001*\"steinbeck\" + 0.001*\"kesterson\" + 0.001*\"kern\" + 0.001*\"hampshir\"\n",
      "\n",
      "\n",
      "Topic: 88 Word: 0.002*\"serb\" + 0.002*\"schwartz\" + 0.001*\"crop\" + 0.001*\"wang\" + 0.001*\"korean\" + 0.001*\"gorazd\" + 0.001*\"bosnian\" + 0.001*\"sarajevo\" + 0.001*\"olympian\" + 0.001*\"mostar\"\n",
      "\n",
      "\n",
      "Topic: 89 Word: 0.003*\"stenholm\" + 0.002*\"ventura\" + 0.002*\"simi\" + 0.002*\"danvill\" + 0.002*\"biolog\" + 0.002*\"idaho\" + 0.001*\"veteran\" + 0.001*\"lake\" + 0.001*\"coast\" + 0.001*\"forest\"\n",
      "\n",
      "\n",
      "Topic: 90 Word: 0.004*\"mexico\" + 0.002*\"nafta\" + 0.002*\"toledo\" + 0.002*\"burk\" + 0.001*\"mancini\" + 0.001*\"indianapoli\" + 0.001*\"haitian\" + 0.001*\"tular\" + 0.001*\"jacob\" + 0.001*\"wilder\"\n",
      "\n",
      "\n",
      "Topic: 91 Word: 0.003*\"brien\" + 0.002*\"loneli\" + 0.001*\"wichita\" + 0.001*\"greenberg\" + 0.001*\"doug\" + 0.001*\"chabot\" + 0.001*\"rochest\" + 0.001*\"albani\" + 0.001*\"sarpalius\" + 0.001*\"eeoc\"\n",
      "\n",
      "\n",
      "Topic: 92 Word: 0.004*\"bilbray\" + 0.004*\"gatt\" + 0.003*\"patent\" + 0.003*\"water\" + 0.003*\"baltimor\" + 0.002*\"minut\" + 0.002*\"investor\" + 0.001*\"vega\" + 0.001*\"florida\" + 0.001*\"bank\"\n",
      "\n",
      "\n",
      "Topic: 93 Word: 0.005*\"oregon\" + 0.004*\"westsid\" + 0.003*\"christensen\" + 0.002*\"bonnevill\" + 0.002*\"nebraska\" + 0.002*\"hanford\" + 0.001*\"oregonian\" + 0.001*\"omaha\" + 0.001*\"portland\" + 0.001*\"rail\"\n",
      "\n",
      "\n",
      "Topic: 94 Word: 0.002*\"pittsburgh\" + 0.002*\"hoosier\" + 0.001*\"pensacola\" + 0.001*\"indiana\" + 0.001*\"hedg\" + 0.001*\"bank\" + 0.001*\"emmitt\" + 0.001*\"indianapoli\" + 0.001*\"welch\" + 0.001*\"anderson\"\n",
      "\n",
      "\n",
      "Topic: 95 Word: 0.002*\"haiti\" + 0.002*\"desert\" + 0.001*\"citadel\" + 0.001*\"charleston\" + 0.001*\"aristid\" + 0.001*\"botan\" + 0.001*\"mississippi\" + 0.001*\"bernardino\" + 0.001*\"mojav\" + 0.001*\"bedel\"\n",
      "\n",
      "\n",
      "Topic: 96 Word: 0.003*\"manton\" + 0.002*\"arizona\" + 0.002*\"queen\" + 0.002*\"nike\" + 0.001*\"delawar\" + 0.001*\"oregon\" + 0.001*\"coppersmith\" + 0.001*\"umatilla\" + 0.001*\"salvag\" + 0.001*\"ireland\"\n",
      "\n",
      "\n",
      "Topic: 97 Word: 0.003*\"island\" + 0.003*\"jersey\" + 0.003*\"polic\" + 0.002*\"unit\" + 0.002*\"crime\" + 0.002*\"turkey\" + 0.002*\"osha\" + 0.002*\"lobbi\" + 0.002*\"resolut\" + 0.002*\"feder\"\n",
      "\n",
      "\n",
      "Topic: 98 Word: 0.003*\"calendar\" + 0.002*\"veteran\" + 0.002*\"raleigh\" + 0.002*\"senat\" + 0.002*\"consent\" + 0.001*\"carolina\" + 0.001*\"unanim\" + 0.001*\"coverag\" + 0.001*\"scranton\" + 0.001*\"health\"\n",
      "\n",
      "\n",
      "Topic: 99 Word: 0.003*\"marshal\" + 0.002*\"tefra\" + 0.002*\"vineland\" + 0.002*\"women\" + 0.001*\"louisvill\" + 0.001*\"clara\" + 0.001*\"hall\" + 0.001*\"walla\" + 0.001*\"chapman\" + 0.001*\"baylor\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print(\"Topic: {} Word: {}\".format(idx, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'prepare_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a3f821728310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepare_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_top_words_per_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_coherence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLDA2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_topics'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import time\n",
    "import shelve\n",
    "\n",
    "import chainer\n",
    "from chainer import cuda\n",
    "from chainer import serializers\n",
    "import chainer.optimizers as O\n",
    "import numpy as np\n",
    "\n",
    "import lda2vec as l2v\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lda2vec import utils, prepare_topics, print_top_words_per_topic, topic_coherence\n",
    "from lda2vec_model import LDA2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-d70cf0ba4b05>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-37-d70cf0ba4b05>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print \"Using GPU \" + str(gpu_id)\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "gpu_id = int(os.getenv('CUDA_GPU', 0))\n",
    "cuda.get_device(gpu_id).use()\n",
    "print \"Using GPU \" + str(gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('data_dir', '../data/')\n",
    "fn_vocab = '{data_dir:s}/vocab.pkl'.format(data_dir=data_dir)\n",
    "fn_corpus = '{data_dir:s}/corpus.pkl'.format(data_dir=data_dir)\n",
    "fn_flatnd = '{data_dir:s}/flattened.npy'.format(data_dir=data_dir)\n",
    "fn_docids = '{data_dir:s}/doc_ids.npy'.format(data_dir=data_dir)\n",
    "fn_vectors = '{data_dir:s}/vectors.npy'.format(data_dir=data_dir)\n",
    "vocab = pickle.load(open(fn_vocab, 'r'))\n",
    "corpus = pickle.load(open(fn_corpus, 'r'))\n",
    "flattened = np.load(fn_flatnd)\n",
    "doc_ids = np.load(fn_docids)\n",
    "vectors = np.load(fn_vectors)\n",
    "\n",
    "# Model Parameters\n",
    "# Number of documents\n",
    "n_docs = doc_ids.max() + 1\n",
    "# Number of unique words in the vocabulary\n",
    "n_vocab = flattened.max() + 1\n",
    "# 'Strength' of the dircihlet prior; 200.0 seems to work well\n",
    "clambda = 200.0\n",
    "# Number of topics to fit\n",
    "n_topics = int(os.getenv('n_topics', 20))\n",
    "batchsize = 4096\n",
    "# Power for neg sampling\n",
    "power = float(os.getenv('power', 0.75))\n",
    "# Intialize with pretrained word vectors\n",
    "pretrained = bool(int(os.getenv('pretrained', True)))\n",
    "# Sampling temperature\n",
    "temperature = float(os.getenv('temperature', 1.0))\n",
    "# Number of dimensions in a single word vector\n",
    "n_units = int(os.getenv('n_units', 300))\n",
    "# Get the string representation for every compact key\n",
    "words = corpus.word_list(vocab)[:n_vocab]\n",
    "# How many tokens are in each document\n",
    "doc_idx, lengths = np.unique(doc_ids, return_counts=True)\n",
    "doc_lengths = np.zeros(doc_ids.max() + 1, dtype='int32')\n",
    "doc_lengths[doc_idx] = lengths\n",
    "# Count all token frequencies\n",
    "tok_idx, freq = np.unique(flattened, return_counts=True)\n",
    "term_frequency = np.zeros(n_vocab, dtype='int32')\n",
    "term_frequency[tok_idx] = freq\n",
    "\n",
    "for key in sorted(locals().keys()):\n",
    "    val = locals()[key]\n",
    "    if len(str(val)) < 100 and '<' not in str(val):\n",
    "        print key, val\n",
    "\n",
    "model = LDA2Vec(n_documents=n_docs, n_document_topics=n_topics,\n",
    "                n_units=n_units, n_vocab=n_vocab, counts=term_frequency,\n",
    "                n_samples=15, power=power, temperature=temperature)\n",
    "if os.path.exists('lda2vec.hdf5'):\n",
    "    print \"Reloading from saved\"\n",
    "    serializers.load_hdf5(\"lda2vec.hdf5\", model)\n",
    "if pretrained:\n",
    "    model.sampler.W.data[:, :] = vectors[:n_vocab, :]\n",
    "model.to_gpu()\n",
    "optimizer = O.Adam()\n",
    "optimizer.setup(model)\n",
    "clip = chainer.optimizer.GradientClipping(5.0)\n",
    "optimizer.add_hook(clip)\n",
    "\n",
    "j = 0\n",
    "epoch = 0\n",
    "fraction = batchsize * 1.0 / flattened.shape[0]\n",
    "progress = shelve.open('progress.shelve')\n",
    "for epoch in range(200):\n",
    "    data = prepare_topics(cuda.to_cpu(model.mixture.weights.W.data).copy(),\n",
    "                          cuda.to_cpu(model.mixture.factors.W.data).copy(),\n",
    "                          cuda.to_cpu(model.sampler.W.data).copy(),\n",
    "                          words)\n",
    "    top_words = print_top_words_per_topic(data)\n",
    "    if j % 100 == 0 and j > 100:\n",
    "        coherence = topic_coherence(top_words)\n",
    "        for j in range(n_topics):\n",
    "            print j, coherence[(j, 'cv')]\n",
    "        kw = dict(top_words=top_words, coherence=coherence, epoch=epoch)\n",
    "        progress[str(epoch)] = pickle.dumps(kw)\n",
    "    data['doc_lengths'] = doc_lengths\n",
    "    data['term_frequency'] = term_frequency\n",
    "    np.savez('topics.pyldavis', **data)\n",
    "    for d, f in utils.chunks(batchsize, doc_ids, flattened):\n",
    "        t0 = time.time()\n",
    "        optimizer.zero_grads()\n",
    "        l = model.fit_partial(d.copy(), f.copy())\n",
    "        prior = model.prior()\n",
    "        loss = prior * fraction\n",
    "        loss.backward()\n",
    "        optimizer.update()\n",
    "        msg = (\"J:{j:05d} E:{epoch:05d} L:{loss:1.3e} \"\n",
    "               \"P:{prior:1.3e} R:{rate:1.3e}\")\n",
    "        prior.to_cpu()\n",
    "        loss.to_cpu()\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        rate = batchsize / dt\n",
    "        logs = dict(loss=float(l), epoch=epoch, j=j,\n",
    "                    prior=float(prior.data), rate=rate)\n",
    "        print msg.format(**logs)\n",
    "        j += 1\n",
    "serializers.save_hdf5(\"lda2vec.hdf5\", model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
